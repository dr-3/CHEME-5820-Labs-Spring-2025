{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14f4219-73df-493c-9d3a-0e01298bae57",
   "metadata": {},
   "source": [
    "# L3b: Classification of Clinical Breast Cancer Samples\n",
    "Linear regression can be adapted for classification tasks by transforming the continuous output of the linear regression model directly to a class designation, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$ or into a probability using an output function $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$. Let's take a look at two examples of these strategies:\n",
    "\n",
    "* [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) is a simple yet powerful algorithm used in machine learning for binary classification tasks. It operates by _incrementally_ learning a linear decision boundary (linear regression model) that separates two classes based on input features by directly mapping the continuous output to a class such as $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$, where the output function is $\\sigma(\\star) = \\texttt{sign}(\\star)$.\n",
    "* [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression#) is a statistical method used in machine learning for binary classification tasks using the [logistics function](https://en.wikipedia.org/wiki/Logistic_function) as the transformation function. Applying the logistic function transforms the output of a linear regression model into a probability, enabling effective decision-making in various applications. We'll consider this approach next time.\n",
    "\n",
    "### Perceptron\n",
    "[The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) takes the (scalar) output of a linear regression model $y_{i}\\in\\mathbb{R}$ and then transforms it using the $\\sigma(\\star) = \\texttt{sign}(\\star)$ function to a discrete set of values representing categories, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$ in the binary classification case. \n",
    "* Suppose there exists a data set\n",
    "$\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ _labeled_ examples, where each example has been labeled by an expert, i.e., a human to be in a category $y_{i}\\in\\{-1,1\\}$, given the $m$-dimensional feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$. \n",
    "* [The Perceptron](https://en.wikipedia.org/wiki/Perceptron) _incrementally_ learns a linear decision boundary between _two_ classes of possible objects (binary classification) by repeatedly processing the dataset $\\mathcal{D}$. During each pass, a regression parameter vector $\\mathbf{\\beta}$ is updated until it makes no more than a specified number of mistakes.  \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) computes the estimated label $\\hat{y}_{i}$ for feature vector $\\hat{\\mathbf{x}}_{i}$ using the $\\texttt{sign}:\\mathbb{R}\\to\\{-1,1\\}$ function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_{i} = \\texttt{sign}\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\cdot\\beta\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\beta=\\left(w_{1},\\dots,w_{n}, b\\right)$ is a column vector of (unknown) classifier parameters, $w_{j}\\in\\mathbb{R}$ corresponding to the importance of feature $j$ and $b\\in\\mathbb{R}$ is a bias parameter, the features $\\hat{\\mathbf{x}}^{\\top}_{i}=\\left(x^{(i)}_{1},\\dots,x^{(i)}_{m}, 1\\right)$ are $p = m+1$-dimensional (row) vectors (features augmented with bias term), and $\\texttt{sign}(z)$ is the function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\texttt{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "__Hypothesis__: [If the dataset $\\mathcal{D}$ is linearly separable](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-LinearSeperableData-Hyperplane.pdf), the Perceptron will _incrementally_ learn a separating hyperplane in a finite number of passes through the data set $\\mathcal{D}$. However, if the [dataset $\\mathcal{D}$ is __not__ linearly separable](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-NotLinearSeperableData-Hyperplane.svg), the Perceptron may not converge. Check out a [perceptron pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf)\n",
    "\n",
    "__Challenge__: We've never seen the dataset in this lab and have no idea if it's linearly separable. Thus, we have no theoretical guarantee that [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) will work. Let's load the dataset, do some preprocessing, and then explore the performance of [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) on this data.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "Before we start, divide into teams and familiarize yourself with the lab. Then, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "\n",
    "* __Task 1: Setup, Data, Constants (10 min)__: Let's take 10 minutes to review the dataset we'll explore today and set up some values we'll use in the other tasks. We'll load the data and do some initial _data munging_ (also called [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling)) to get the dataset in a form that we'll use in our analysis.\n",
    "* __Task 2: Build a Classification Model and Learn the Parameters (10 min)__: In this task, we'll build a model of our classification problem and train the model using an online learning method.\n",
    "* __Task 3: Classify the test data and compute the Confusion matrix (20 min)?__: In this task, we'll use the updated `model::MyPerceptronClassificationModel` instance (that has learned its parameters from the `training` data in _Task 2_) and test how well we classify data that we have never seen, i.e., how well we classify the `test` dataset. We'll then compute [the confusion matrix](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf) which we can use to test how well the classifier is working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ad205-611d-47eb-bbc0-c69736429a47",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447d14fd-baf9-47f6-bafe-4c1839460fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd8452-683e-462e-ade3-39edb8c22e37",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this lab, we'll use [the Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) to classify clinical Breast Cancer samples taken from the University of Wisconsin. \n",
    "* __Description__: The breast cancer dataset developed by [Wolberg, W. (1990)](https://doi.org/10.24432/C5HP4Z) was obtained from the University of Wisconsin Hospitals, Madison, from [Dr. William H. Wolberg](https://pages.cs.wisc.edu/~olvi/uwmp/cancer.html), and is available [from the UCI dataset archive](https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original). It contains `699` instances, with `9` clinical features and a class label `{benign | malignant}.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bd5500-6f9a-4b19-b1c0-0adf61b0ab0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>699Ã—11 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">674 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">id</th><th style = \"text-align: left;\">ClumpThickness</th><th style = \"text-align: left;\">UniformityCellSize</th><th style = \"text-align: left;\">UniformityCellShape</th><th style = \"text-align: left;\">MarginalAdhesion</th><th style = \"text-align: left;\">SingleEpithelialCellSize</th><th style = \"text-align: left;\">BareNuclei</th><th style = \"text-align: left;\">BlandChromatin</th><th style = \"text-align: left;\"> NormalNucleoli</th><th style = \"text-align: left;\">Mitoses</th><th style = \"text-align: left;\">Class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1000025</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1002945</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1015425</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1016277</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1017023</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1017122</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">1018099</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">1018561</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1035283</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1036172</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1041801</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">688</td><td style = \"text-align: right;\">566346</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">689</td><td style = \"text-align: right;\">603148</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">690</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">691</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">692</td><td style = \"text-align: right;\">695091</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">693</td><td style = \"text-align: right;\">714039</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">694</td><td style = \"text-align: right;\">763235</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">695</td><td style = \"text-align: right;\">776715</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">696</td><td style = \"text-align: right;\">841769</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">697</td><td style = \"text-align: right;\">888820</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">698</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">699</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& id & ClumpThickness & UniformityCellSize & UniformityCellShape & MarginalAdhesion & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1000025 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t2 & 1002945 & 5 & 4 & 4 & 5 & $\\dots$ \\\\\n",
       "\t3 & 1015425 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t4 & 1016277 & 6 & 8 & 8 & 1 & $\\dots$ \\\\\n",
       "\t5 & 1017023 & 4 & 1 & 1 & 3 & $\\dots$ \\\\\n",
       "\t6 & 1017122 & 8 & 10 & 10 & 8 & $\\dots$ \\\\\n",
       "\t7 & 1018099 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t8 & 1018561 & 2 & 1 & 2 & 1 & $\\dots$ \\\\\n",
       "\t9 & 1033078 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t10 & 1033078 & 4 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t11 & 1035283 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t12 & 1036172 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t13 & 1041801 & 5 & 3 & 3 & 3 & $\\dots$ \\\\\n",
       "\t14 & 1043999 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t15 & 1044572 & 8 & 7 & 5 & 10 & $\\dots$ \\\\\n",
       "\t16 & 1047630 & 7 & 4 & 6 & 4 & $\\dots$ \\\\\n",
       "\t17 & 1048672 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t18 & 1049815 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t19 & 1050670 & 10 & 7 & 7 & 6 & $\\dots$ \\\\\n",
       "\t20 & 1050718 & 6 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t21 & 1054590 & 7 & 3 & 2 & 10 & $\\dots$ \\\\\n",
       "\t22 & 1054593 & 10 & 5 & 5 & 3 & $\\dots$ \\\\\n",
       "\t23 & 1056784 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t24 & 1057013 & 8 & 4 & 5 & 1 & $\\dots$ \\\\\n",
       "\t25 & 1059552 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t26 & 1065726 & 5 & 2 & 3 & 4 & $\\dots$ \\\\\n",
       "\t27 & 1066373 & 3 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t28 & 1066979 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t29 & 1067444 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t30 & 1070935 & 1 & 1 & 3 & 1 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m699Ã—11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0mâ”‚\u001b[1m id      \u001b[0m\u001b[1m ClumpThickness \u001b[0m\u001b[1m UniformityCellSize \u001b[0m\u001b[1m UniformityCellShape \u001b[0m\u001b[1m Margi\u001b[0m â‹¯\n",
       "     â”‚\u001b[90m Int64   \u001b[0m\u001b[90m Int64          \u001b[0m\u001b[90m Int64              \u001b[0m\u001b[90m Int64               \u001b[0m\u001b[90m Int64\u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "   1 â”‚ 1000025               5                   1                    1        â‹¯\n",
       "   2 â”‚ 1002945               5                   4                    4\n",
       "   3 â”‚ 1015425               3                   1                    1\n",
       "   4 â”‚ 1016277               6                   8                    8\n",
       "   5 â”‚ 1017023               4                   1                    1        â‹¯\n",
       "   6 â”‚ 1017122               8                  10                   10\n",
       "   7 â”‚ 1018099               1                   1                    1\n",
       "   8 â”‚ 1018561               2                   1                    2\n",
       "   9 â”‚ 1033078               2                   1                    1        â‹¯\n",
       "  10 â”‚ 1033078               4                   2                    1\n",
       "  11 â”‚ 1035283               1                   1                    1\n",
       "  â‹®  â”‚    â‹®           â‹®                 â‹®                    â‹®                 â‹±\n",
       " 690 â”‚  654546               1                   1                    1\n",
       " 691 â”‚  654546               1                   1                    1        â‹¯\n",
       " 692 â”‚  695091               5                  10                   10\n",
       " 693 â”‚  714039               3                   1                    1\n",
       " 694 â”‚  763235               3                   1                    1\n",
       " 695 â”‚  776715               3                   1                    1        â‹¯\n",
       " 696 â”‚  841769               2                   1                    1\n",
       " 697 â”‚  888820               5                  10                   10\n",
       " 698 â”‚  897471               4                   8                    6\n",
       " 699 â”‚  897471               4                   8                    8        â‹¯\n",
       "\u001b[36m                                                  7 columns and 678 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.read(joinpath(_PATH_TO_DATA, \"breast-cancer-wisconsin.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119421d2-c887-450c-b7c4-34e0e29b98dd",
   "metadata": {},
   "source": [
    "__Data wrangling__: The `Class` label is not in the form of $\\{-1,1\\}$ that the perceptron expects, so let's transform the original data where we map $2\\rightarrow{-1}$ and $4\\rightarrow{1}$. We'll save the transformed data in the `dataset::DataFrame` variable. In our transformed dataset the `-1` label is _not cancer_ `benign` while the label `1` denotes _cancer_ or `malignant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d90a573-7377-4390-807b-13268b3bd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>699Ã—11 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">674 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">id</th><th style = \"text-align: left;\">ClumpThickness</th><th style = \"text-align: left;\">UniformityCellSize</th><th style = \"text-align: left;\">UniformityCellShape</th><th style = \"text-align: left;\">MarginalAdhesion</th><th style = \"text-align: left;\">SingleEpithelialCellSize</th><th style = \"text-align: left;\">BareNuclei</th><th style = \"text-align: left;\">BlandChromatin</th><th style = \"text-align: left;\"> NormalNucleoli</th><th style = \"text-align: left;\">Mitoses</th><th style = \"text-align: left;\">Class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1000025</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1002945</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1015425</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1016277</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1017023</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1017122</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">1018099</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">1018561</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1035283</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1036172</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1041801</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">688</td><td style = \"text-align: right;\">566346</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">689</td><td style = \"text-align: right;\">603148</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">690</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">691</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">692</td><td style = \"text-align: right;\">695091</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">693</td><td style = \"text-align: right;\">714039</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">694</td><td style = \"text-align: right;\">763235</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">695</td><td style = \"text-align: right;\">776715</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">696</td><td style = \"text-align: right;\">841769</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">697</td><td style = \"text-align: right;\">888820</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">698</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">699</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& id & ClumpThickness & UniformityCellSize & UniformityCellShape & MarginalAdhesion & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1000025 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t2 & 1002945 & 5 & 4 & 4 & 5 & $\\dots$ \\\\\n",
       "\t3 & 1015425 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t4 & 1016277 & 6 & 8 & 8 & 1 & $\\dots$ \\\\\n",
       "\t5 & 1017023 & 4 & 1 & 1 & 3 & $\\dots$ \\\\\n",
       "\t6 & 1017122 & 8 & 10 & 10 & 8 & $\\dots$ \\\\\n",
       "\t7 & 1018099 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t8 & 1018561 & 2 & 1 & 2 & 1 & $\\dots$ \\\\\n",
       "\t9 & 1033078 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t10 & 1033078 & 4 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t11 & 1035283 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t12 & 1036172 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t13 & 1041801 & 5 & 3 & 3 & 3 & $\\dots$ \\\\\n",
       "\t14 & 1043999 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t15 & 1044572 & 8 & 7 & 5 & 10 & $\\dots$ \\\\\n",
       "\t16 & 1047630 & 7 & 4 & 6 & 4 & $\\dots$ \\\\\n",
       "\t17 & 1048672 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t18 & 1049815 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t19 & 1050670 & 10 & 7 & 7 & 6 & $\\dots$ \\\\\n",
       "\t20 & 1050718 & 6 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t21 & 1054590 & 7 & 3 & 2 & 10 & $\\dots$ \\\\\n",
       "\t22 & 1054593 & 10 & 5 & 5 & 3 & $\\dots$ \\\\\n",
       "\t23 & 1056784 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t24 & 1057013 & 8 & 4 & 5 & 1 & $\\dots$ \\\\\n",
       "\t25 & 1059552 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t26 & 1065726 & 5 & 2 & 3 & 4 & $\\dots$ \\\\\n",
       "\t27 & 1066373 & 3 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t28 & 1066979 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t29 & 1067444 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t30 & 1070935 & 1 & 1 & 3 & 1 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m699Ã—11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0mâ”‚\u001b[1m id      \u001b[0m\u001b[1m ClumpThickness \u001b[0m\u001b[1m UniformityCellSize \u001b[0m\u001b[1m UniformityCellShape \u001b[0m\u001b[1m Margi\u001b[0m â‹¯\n",
       "     â”‚\u001b[90m Int64   \u001b[0m\u001b[90m Int64          \u001b[0m\u001b[90m Int64              \u001b[0m\u001b[90m Int64               \u001b[0m\u001b[90m Int64\u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "   1 â”‚ 1000025               5                   1                    1        â‹¯\n",
       "   2 â”‚ 1002945               5                   4                    4\n",
       "   3 â”‚ 1015425               3                   1                    1\n",
       "   4 â”‚ 1016277               6                   8                    8\n",
       "   5 â”‚ 1017023               4                   1                    1        â‹¯\n",
       "   6 â”‚ 1017122               8                  10                   10\n",
       "   7 â”‚ 1018099               1                   1                    1\n",
       "   8 â”‚ 1018561               2                   1                    2\n",
       "   9 â”‚ 1033078               2                   1                    1        â‹¯\n",
       "  10 â”‚ 1033078               4                   2                    1\n",
       "  11 â”‚ 1035283               1                   1                    1\n",
       "  â‹®  â”‚    â‹®           â‹®                 â‹®                    â‹®                 â‹±\n",
       " 690 â”‚  654546               1                   1                    1\n",
       " 691 â”‚  654546               1                   1                    1        â‹¯\n",
       " 692 â”‚  695091               5                  10                   10\n",
       " 693 â”‚  714039               3                   1                    1\n",
       " 694 â”‚  763235               3                   1                    1\n",
       " 695 â”‚  776715               3                   1                    1        â‹¯\n",
       " 696 â”‚  841769               2                   1                    1\n",
       " 697 â”‚  888820               5                  10                   10\n",
       " 698 â”‚  897471               4                   8                    6\n",
       " 699 â”‚  897471               4                   8                    8        â‹¯\n",
       "\u001b[36m                                                  7 columns and 678 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = let\n",
    "\n",
    "    number_of_examples = nrow(df);\n",
    "    for i âˆˆ 1:number_of_examples\n",
    "        c = df[i,:Class];\n",
    "        if (c == 2)\n",
    "            df[i,:Class] = -1 # not cancer\n",
    "        elseif (c == 4)\n",
    "            df[i,:Class] = 1 # cancer\n",
    "        end\n",
    "    end\n",
    "    df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390f5e1-d65f-4e75-be89-be65fd09fa5f",
   "metadata": {},
   "source": [
    "Next, let's (randomly) partition the clinical data into `training` and `test` sets. \n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data. In our case, we'll use the training data to estimate the classifier parameters $\\beta$.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2807cc0-14b1-4e55-9547-8ec1c71d86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 456; # from Sidey-Gibbons, 2019\n",
    "    D = Matrix(dataset);\n",
    "    number_of_features = size(D,2); # number of cols of housing data\n",
    "    number_of_examples = size(D,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5d7e389-4b31-4458-87a3-dddee59c8707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456Ã—11 Matrix{Int64}:\n",
       " 1111249  10   6   6   3  4   5   3   6  1   1\n",
       " 1223967   6   1   3   1  2   1   3   1  1  -1\n",
       "  897471   4   8   8   5  4   5  10   4  1   1\n",
       " 1222047  10  10  10  10  3  10  10   6  1   1\n",
       " 1354840   2   1   1   1  2   1   3   1  1  -1\n",
       " 1124651   1   3   3   2  2   1   7   2  1  -1\n",
       "  718641   1   1   1   1  5   1   3   1  1  -1\n",
       "  183913   1   2   2   1  2   1   1   1  1  -1\n",
       " 1330361   5   1   1   1  2   1   2   1  1  -1\n",
       "  536708   1   1   1   1  2   1   1   1  1  -1\n",
       " 1171845   8   6   4   3  5   9   3   1  1   1\n",
       "  831268   1   1   1   1  1   1   1   3  1  -1\n",
       " 1205579   8   7   6   4  4  10   5   1  1   1\n",
       "       â‹®                  â‹®                  â‹®\n",
       " 1183596   3   1   3   1  3   4   1   1  1  -1\n",
       " 1266124   5   1   2   1  2   1   1   1  1  -1\n",
       " 1164066   1   1   1   1  2   1   3   1  1  -1\n",
       " 1238777   6   1   1   3  2   1   1   1  1  -1\n",
       " 1187457   3   1   1   3  8   1   5   8  1  -1\n",
       " 1190485   1   1   1   1  2   1   1   1  1  -1\n",
       " 1285722   4   1   1   3  2   1   1   1  1  -1\n",
       " 1311875   5   1   2   1  2   1   1   1  1  -1\n",
       " 1203096   1   1   1   1  1   1   3   1  1  -1\n",
       " 1265899   4   1   1   1  2   1   3   1  1  -1\n",
       "  563649   8   8   8   1  2   0   6  10  1   1\n",
       " 1204558   4   1   1   1  2   1   2   1  1  -1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89207eca-49a5-4ac0-b4f7-7d884534d52e",
   "metadata": {},
   "source": [
    "## Task 2: Build a Classification Model and Learn the Parameters\n",
    "In this task, we'll build a model of our classification problem and train the model using an online learning method. \n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem. We initialize the parameters using a vector of `1`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ee2374-edba-46d5-8195-7ad63e8770ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # How many features do we have?\n",
    "    D = training; # let's look at the training data\n",
    "    number_of_features = size(D,2) - 1; # why minus one?\n",
    "    \n",
    "    # build a model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee0f62-b682-4d7f-b1c2-183fc50cff78",
   "metadata": {},
   "source": [
    "Next, we then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35eb0bee-44e1-4251-b91c-e78637447d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 15\n"
     ]
    }
   ],
   "source": [
    "trainedmodel = let\n",
    "\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2) - 1; # how many features do we have (cols)?\n",
    "    X = [D[:,2:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # train the model -\n",
    "    trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return\n",
    "    trainedmodel;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cf5f9-fc9b-4bff-9759-385386553fa1",
   "metadata": {},
   "source": [
    "__Hmmmm__: Given the exit message above, can we say if this dataset is linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f824e-d5ce-4019-949c-980c73da15f9",
   "metadata": {},
   "source": [
    "## Task 3: Classify the test data and compute the Confusion matrix\n",
    "In this task, we'll use the updated `model::MyPerceptronClassificationModel` instance (that has learned some parameters from the `training` data) and test how well we classify data that we have never seen, i.e., how well we classify the `test` dataset.\n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y::Array{Int64,1}` vector, while the model predicted label is stored in the `yÌ‚::Array{Int64,1}` array.\n",
    "* __Performance__: Once the Perceptron (or any binary classifier) has converged, we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the data set $\\mathcal{D}$. \n",
    "Various metrics can be used to evaluate the performance of a binary classifier, but they all start with computing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4353492b-c1ea-4620-89c2-595da58bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚,y = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2) - 1; # how many features do we have (cols)?\n",
    "    X = [D[:,2:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    yÌ‚ = classify(X,model)\n",
    "\n",
    "    # return -\n",
    "    yÌ‚,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b5acc-0d2f-4d01-84c1-3b01a25bc922",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf) The four cases are:\n",
    "* The __true positive (TP)__ case $(\\text{actual}, \\text{model}) = (+,+)$ in the confusion matrix is the number of positive examples that were correctly classified as positive.\n",
    "* The __false negative (FN)__ case $(\\text{actual}, \\text{model}) = (+,-)$ is the number of actual positive examples the model incorrectly classified as negative.\n",
    "* The __false positive (FP)__ case $(\\text{actual}, \\text{model}) = (-,+)$ is the number of actual negative examples that were incorrectly classified as positive by the model.\n",
    "* The __true negative (TN)__ case $(\\text{actual}, \\text{model}) = (-,-)$ is the number of actual negative examples that were correctly classified as negative by the model.\n",
    "\n",
    "Let's compute these four values and store them in the `confusion_matrix::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e3935c-f6db-474d-9f8d-0c095b1543b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2Ã—2 Matrix{Int64}:\n",
       " 77    1\n",
       "  9  156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_test_examples = length(yÌ‚);\n",
    "    confusion_matrix = Array{Int64,2}(undef,2,2); # 2 x 2 array\n",
    "\n",
    "    # True positive: TP (cancer)\n",
    "    counter = 0;\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        if (y[i] == 1 && yÌ‚[i] == 1)\n",
    "            counter+=1;\n",
    "        end\n",
    "    end\n",
    "    confusion_matrix[1,1] = counter;\n",
    "\n",
    "    # False negative: FN\n",
    "    counter = 0;\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        if (y[i] == 1 && yÌ‚[i] == -1)\n",
    "            counter+=1;\n",
    "        end\n",
    "    end\n",
    "    confusion_matrix[1,2] = counter;\n",
    "\n",
    "    # False position: FP\n",
    "    counter = 0;\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        if (y[i] == -1 && yÌ‚[i] == 1)\n",
    "            counter+=1;\n",
    "        end\n",
    "    end\n",
    "    confusion_matrix[2,1] = counter;\n",
    "\n",
    "    # True negative: TN\n",
    "    counter = 0;\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        if (y[i] == -1 && yÌ‚[i] == -1)\n",
    "            counter+=1;\n",
    "        end\n",
    "    end\n",
    "    confusion_matrix[2,2] = counter;\n",
    "\n",
    "    # return -\n",
    "    confusion_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ef507-9759-4d5b-bfd4-70f55f122b35",
   "metadata": {},
   "source": [
    "### Discussion questions\n",
    "Let's answer some questions using the confusion matrix about how well the Perceptron worked, i.e., how well it correctly predicted the class label `{benign | malignant}` given the clinical features.\n",
    "1. What fraction of the `test` examples did the classifier agent get correct, i.e., the observed label was correctly predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c5ef816-cf29-4271-8ce5-5627dfcd84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_samples = length(yÌ‚);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bdfa866-6d9f-4244-8200-5a8138849574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.9588477366255144\n"
     ]
    }
   ],
   "source": [
    "TC = confusion_matrix[1,1] + confusion_matrix[2,2];\n",
    "TC/number_of_test_samples |> f-> println(\"Fraction correct: $(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b849e-e46b-40d8-a0a7-a1a3b8c758ce",
   "metadata": {},
   "source": [
    "2. For those cases where the classifier was wrong, is it biased toward being (incorrectly) positive or (incorrecty) negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad98768d-c1e4-4a24-b8e5-fd3af774c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = confusion_matrix[2,1] + confusion_matrix[1,2]; # total wrong\n",
    "FN = confusion_matrix[1,2]; # (+,-)\n",
    "FP = confusion_matrix[2,1]; # (-,+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4c17ae9-b81d-4b90-94f2-cdd60402353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negative rate 0.1\n"
     ]
    }
   ],
   "source": [
    "println(\"False negative rate $(FN/TW)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d201a694-1d28-4c5f-a1f0-653312ac5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate 0.9\n"
     ]
    }
   ],
   "source": [
    "println(\"False positive rate $(FP/TW)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5b02f-bed5-4231-8392-5242543e1b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
