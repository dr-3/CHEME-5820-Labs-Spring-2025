{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae26db5-5215-4b26-b425-521f797a9b9a",
   "metadata": {},
   "source": [
    "# Lab 3d: The XOR Problem\n",
    "In this lab, we'll look at the performance of [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) and [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) on a dataset that is _not linearly seperable_, namely [the XOR dataset](https://en.wikipedia.org/wiki/Exclusive_or).\n",
    "\n",
    "* __Backstory__: In 1969, AI researchers [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) published a book called [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)), in which they argued that [the perceptron built by Rosenblatt](https://en.wikipedia.org/wiki/Perceptron) was incapable of learning certain functions, one of those being [the XOR function](https://en.wikipedia.org/wiki/Exclusive_or). Soon after [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)) was published, the first [AI winter](https://en.wikipedia.org/wiki/AI_winter) began. Are these events correlated, maybe!\n",
    "\n",
    "First, let's build and analyze datasets that _should work_, i.e., they are (nearly) linearly separable, and then we'll explore datasets that we know for sure _will not work_, i.e., they are not linearly separable, and see what happens.  \n",
    "\n",
    "### Tasks\n",
    "Before we start, divide into teams and familiarize yourself with the lab. Then, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "\n",
    "* __Task 1: Setup, Data, Constants (10 min)__: Let's take 10 minutes to explore how we will generate the datasets we'll explore today. We'll work through how to generate linearly separable and non-linearly separable datasets.\n",
    "* __Task 2: Build and Train Perceptron Classification Model (20 min)__: In this task, we'll build and train a Perceptron classification model, use the trained model to estimate the labels on unseen test data, and then compute the confusion matrix.\n",
    "* __Task 3:  Build and Train Logistic Regression Classification Model (20 min)?__: In this task, we'll build and train a Logistic regression classification model, use the trained model to estimate the labels on unseen test data, and then compute the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af49309-7d3d-47c7-a48d-af09abe2976d",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "In this task, we set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "\n",
    "The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b32458-bba8-4073-b4ea-39b33c95d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917cbd8-6f10-4079-92a3-8429d35ced1d",
   "metadata": {},
   "source": [
    "Next, let's set some constants we'll need for the data generation logic below. Please look at the comment next to the constant for a description of what it is, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861bf71e-73d3-4c38-8394-9ce4ab95ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_label_one = 1000; # number of points in cloud 1 (must be ≥ 2)\n",
    "number_label_two = 1000; # number of points in cloud 2 (must be ≥ 2)\n",
    "total_number_of_points = (number_label_one + number_label_two);\n",
    "number_of_features = 3; # features: (x,y,l), where l is a generated label; see below.\n",
    "c̄₁ = (0.0, 0.0); # center for the cloud: fixed\n",
    "θ = 60*(π/180); # rotation angle (radians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3a812-6f24-4130-bffc-77019b4359f8",
   "metadata": {},
   "source": [
    "Finally, let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary class labels, i.e., $ y\\in\\{1,-1\\}$ while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7243d3-ecd5-4975-822b-cd03cca324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a14d49-485e-4db4-8866-b314d028d42a",
   "metadata": {},
   "source": [
    "### Data\n",
    "We'll use [the Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) and [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) to classify datasets, including [the XOR dataset](https://en.wikipedia.org/wiki/Exclusive_or) that we construct. First, we'll generate a master dataset (which may or may not be linearly separable), and then we'll split it into `training` and `test` subsets.\n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data. In our case, we'll use the training data to estimate the classifier parameters $\\beta$.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_.\n",
    "\n",
    "Let's start with the master dataset `D::Array{Float64,2}`. This dataset will have two continuous features $\\mathbf{x}\\in\\mathbb{R}^{2}$ and a categorical label $y\\in\\{-1,1\\}$. We'll build a label function $L:\\mathbb{R}\\times\\mathbb{R}\\to\\{\\text{true, false}\\}$ (we can change this function around to get different labeling patterns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f04981-afa6-4add-b1e0-ad9d9d4ae8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment this logic for different labeling patterns\n",
    "# L(x,y) = (y ≥ 0) ? true : false; # rotated half circle (linearly separable)\n",
    "# L(x,y) = (x ≥ 0) && (y ≥ 0) ? true : false; # wedge pattern (not linearly separable)\n",
    "# L(x,y) = xor(x ≥ 0,y ≥ 0); # XOR pattern alternating pie wedges (not linearly separable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2390535-88f3-4047-ac22-3ab0708b651b",
   "metadata": {},
   "source": [
    "Generate the master dataset `D`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503ba9b1-4aea-4141-a29a-87d1416d8ceb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `L` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `L` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[11]:20"
     ]
    }
   ],
   "source": [
    "D = let\n",
    "    \n",
    "    # initialize -\n",
    "    D = Array{Float64,2}(undef, total_number_of_points, number_of_features);\n",
    "    s₁ = generatedatacloud(c̄₁, number_of_points = number_label_one, label=0); # generate label 1 data\n",
    "    s₂ = generatedatacloud(c̄₁, number_of_points = number_label_two, label=0); # generate label 2 data\n",
    "\n",
    "    # mix s₁, s₂ together (randomly)\n",
    "    tmp = vcat(s₁,s₂)\n",
    "    random_perm_index_vector = randperm(total_number_of_points);\n",
    "    for i ∈ eachindex(random_perm_index_vector)\n",
    "        k = random_perm_index_vector[i]; # get the from col -\n",
    "        for j ∈ 1:number_of_features\n",
    "            D[i,j] = tmp[k,j];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # compute the label by taking the XOR, OR, AND, etc functions\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        flag = L(D[i,1],D[i,2]);\n",
    "        if (flag == true)\n",
    "            D[i,3] = 1\n",
    "        else\n",
    "            D[i,3] = -1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # finally, let's rotate the data a bit (counter-clockwise)\n",
    "    R = [\n",
    "        cos(θ) -sin(θ) ;\n",
    "        sin(θ) cos(θ) ;\n",
    "    ];\n",
    "\n",
    "    # keep the label, but apply the rotation the (x,y) data\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        x̂ = R*D[i,1:2];\n",
    "        D̂[i,1] = x̂[1];\n",
    "        D̂[i,2] = x̂[2];\n",
    "    end\n",
    "    \n",
    "    D̂ # return the data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920e5a9-440a-48b8-85a8-c2132eec7358",
   "metadata": {},
   "source": [
    "#### Visualize dataset `D`\n",
    "`Unhide` the code block below to see how we plotted the dataset `D` which contains two continuous features and a label. The color indicates the label.\n",
    "* __Summary__: We will get a different pattern of $\\pm{1}$ labels depending on the labeling function logic $L(x_{1},x_{2})$ we used. The dark blue dots represent label `1`, while the orange data represents label `1`. Our \n",
    "classifier should be able to learn the mapping between the features and the labels for linearly separable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a9ec21-6a1b-46e6-b9ae-eac967bfe340",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[13]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    dataset = D; # what dataset am I looking at?\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "\n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot all points\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        label = dataset[i,3]; # label\n",
    "        c = my_color_dictionary[label]\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "    \n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63663e3-b83d-4949-b524-778057f436c5",
   "metadata": {},
   "source": [
    "Next, let's split that dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef6d985-ebcb-486f-a7ce-f9117a438c7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[15]:21"
     ]
    }
   ],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 1200; # we make this number of\n",
    "    number_of_examples = total_number_of_points;\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299ad3d-0b54-44c3-856c-9d9fcbfbe955",
   "metadata": {},
   "source": [
    "__Visualize__ the `training` of the `test` datasets. These should look like the original dataset `D`, just with fewer elements, i.e., we should _not_ see an overrepresentation of a particular label because we selected the datasets at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7974ffd7-1bbf-4466-8040-e0e38b2926e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[17]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    dataset = training; # what dataset am I looking at?\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "\n",
    "    caselabel = \"training\";\n",
    "    if (dataset == test)\n",
    "        caselabel=\"test\";\n",
    "    end\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    \n",
    "    for i ∈ 1:number_of_points\n",
    "        label = dataset[i,3]; # label\n",
    "        c = my_color_dictionary[label]\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Case: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6120fb0-030b-4412-9bd9-51d4c0e1e440",
   "metadata": {},
   "source": [
    "## Task 2: Build a Perceptron Classification Model and Learn the Parameters\n",
    "In this task, we'll build a model of our classification problem and train the model using an online learning method. \n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem. We initialize the parameters using a vector of `1`'s.\n",
    "* Next, we then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b466172-602e-4920-adf7-4102a04f1db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[19]:4"
     ]
    }
   ],
   "source": [
    "perceptron_model = let\n",
    "    \n",
    "    # setup\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # build an initial model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # TODO: uncomment me to train the model -\n",
    "    # trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    trainedmodel;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144eb6b-0a7d-4a19-a12f-8e91c82750ff",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between classes on data it has never seen. \n",
    "* We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `ŷ_perceptron::Array{Int64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "631c058f-7dc1-4848-971e-0f4190476912",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `test` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `test` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[21]:3"
     ]
    }
   ],
   "source": [
    "ŷ_perceptron,y_perceptron = let\n",
    "\n",
    "    D = test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # TODO: uncomment to compute the estimated labels -\n",
    "    # ŷ = classify(X,perceptron_model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5d039-a82d-4971-a295-6da45c7cf86d",
   "metadata": {},
   "source": [
    "__Confusion matrix__: The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). \n",
    "\n",
    "Let's compute the confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_perceptron::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a74f804-9a19-468e-814a-0ab6d58abbc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[23]:1"
     ]
    }
   ],
   "source": [
    "CM_perceptron = confusion(y_perceptron, ŷ_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93612c6-f1bb-46fa-be08-b891b708a554",
   "metadata": {},
   "source": [
    "Finally, we can compute the overall error rate for the perceptron (or other performance metrics) using values from [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b72ea59-b9e4-44d9-8517-d4eda6d9558e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[25]:1"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_perceptron = CM_perceptron[1,1] + CM_perceptron[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fab6c-776d-4cb4-b7ab-f789f0e0cab5",
   "metadata": {},
   "source": [
    "__Visualize the misses__. Using the test dataset, let's show (with gray circles) which samples our classifier is unable to predict the label correctly, i.e., where we miss the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d86717ea-39e1-4918-8cef-c2daa78fa57d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `perceptron_model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `perceptron_model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[27]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    model = perceptron_model; # which model am I using?\n",
    "    dataset = test; # what dataset am I looking at?\n",
    "    caselabel = \"test\";\n",
    "    actual = y_perceptron;\n",
    "    predicted = ŷ_perceptron;\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # let's draw the separating hyperplane (in our case, a line)\n",
    "    p = model.β;\n",
    "    number_of_plane_points = 200;\n",
    "    x₂ = zeros(number_of_plane_points);\n",
    "    x₁ = range(-1,stop=1,length = number_of_plane_points) |> collect;\n",
    "    for i ∈ 1:number_of_plane_points\n",
    "        x₂[i] = -1*((p[1]/p[2])*x₁[i] + p[3]/p[2]);\n",
    "    end\n",
    "    plot!(x₁,x₂,lw=2, c=:green, label=\"Learned boundary\")\n",
    "    \n",
    "    # data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        actuallabel = actual[i]; # actual label\n",
    "        testlabel = predicted[i]; # predited label\n",
    "\n",
    "        c = :gray60;\n",
    "        if (actuallabel == testlabel)\n",
    "            c = my_color_dictionary[actuallabel]\n",
    "        end\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Perceptron: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5a28f-e15f-4e82-a956-a2a7eb8636a4",
   "metadata": {},
   "source": [
    "## Task 3: Build and Train Logistic Regression Classification Model\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset.\n",
    "\n",
    "__Key differences__: Unlike the Perceptron model, which outputs the class label directly, logistic regression models compute the _probability_ that a given input belongs to a particular class based on the input features. The training method is also different, we have to iteratively estimate the model parameters (in this case using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)).\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](src/Types.jl), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: We approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). This is generally not a great idea. This is one of my super pet peeves with gradient descent; computing the gradient is (usually) a hassle. Typically, we have to do at least two function evaluations to approximate the gradient well. Why do finite diference? It is easy to implement.\n",
    "* In the code below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](src/Factory.jl). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](src/Compute.jl), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf482f20-69dc-479d-99da-4ec5e18ca0c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[29]:4"
     ]
    }
   ],
   "source": [
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    D = training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.01, # you pick this\n",
    "        ϵ = 1e-4, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log10(1+exp(-y*(dot(x,θ)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # TDOD: uncomment below to train the Logistic model\n",
    "    # model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbd99c-9b57-430a-97c5-ebd4ce0bc468",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (with parameters learned from the `training` data) and test how well we can classify data in the `test` dataset.\n",
    "\n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "* We store the actual (correct) label in the `y_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_logistic::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "045a7e5f-35fc-4e8c-8099-d77ee9fa9f40",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `test` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `test` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[31]:3"
     ]
    }
   ],
   "source": [
    "ŷ_logistic,y_logistic, P = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # TODO: Uncomment below to compute the estimated labels -\n",
    "    # P = classify(X,model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275e426-1eee-49d5-8f96-d4c3901991f7",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged (or exhasted our iterations), we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "* We compute confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_logistic::Array{Int64,2}` variable. The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f824b551-3053-4e9f-bad0-879fd1bd3d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y_logistic` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_logistic` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[33]:1"
     ]
    }
   ],
   "source": [
    "CM_logistic = confusion(y_logistic, ŷ_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89da06-6f5d-4812-96a6-706bd1ca7022",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the logistic regression using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d61d24f-eba9-4138-9d36-61f3f31e188b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_perceptron` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[35]:1"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2];\n",
    "(correct_prediction_logistic/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef62a4cd-9e1d-4806-9e41-0c7a13f7d584",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `model_logistic` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `model_logistic` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[36]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    model = model_logistic; # which model am I using?\n",
    "    dataset = test; # what dataset am I looking at?\n",
    "    caselabel = \"test\";\n",
    "    actual = y_logistic;\n",
    "    predicted = ŷ_logistic;\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # let's draw the separating hyperplane (in our case, a line)\n",
    "    p = model.β;\n",
    "    number_of_plane_points = 200;\n",
    "    x₂ = zeros(number_of_plane_points);\n",
    "    x₁ = range(-1,stop=1,length = number_of_plane_points) |> collect;\n",
    "    for i ∈ 1:number_of_plane_points\n",
    "        x₂[i] = -1*((p[1]/p[2])*x₁[i] + p[3]/p[2]);\n",
    "    end\n",
    "    plot!(x₁,x₂,lw=2, c=:green, label=\"Learned boundary\")\n",
    "    \n",
    "    # data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        actuallabel = actual[i]; # actual label\n",
    "        testlabel = predicted[i]; # predited label\n",
    "\n",
    "        c = :gray60;\n",
    "        if (actuallabel == testlabel)\n",
    "            c = my_color_dictionary[actuallabel]\n",
    "        end\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Logistic: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
